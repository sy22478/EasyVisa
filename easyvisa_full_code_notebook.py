# -*- coding: utf-8 -*-
"""EasyVisa_Full_Code_Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EB1ZMzJt8WUsu-43OVbpFLWGwWhqvPZg

## Problem Statement

### Business Context

Business communities in the United States are facing high demand for human resources, but one of the constant challenges is identifying and attracting the right talent, which is perhaps the most important element in remaining competitive. Companies in the United States look for hard-working, talented, and qualified individuals both locally as well as abroad.

The Immigration and Nationality Act (INA) of the US permits foreign workers to come to the United States to work on either a temporary or permanent basis. The act also protects US workers against adverse impacts on their wages or working conditions by ensuring US employers' compliance with statutory requirements when they hire foreign workers to fill workforce shortages. The immigration programs are administered by the Office of Foreign Labor Certification (OFLC).

OFLC processes job certification applications for employers seeking to bring foreign workers into the United States and grants certifications in those cases where employers can demonstrate that there are not sufficient US workers available to perform the work at wages that meet or exceed the wage paid for the occupation in the area of intended employment.

### Objective

In FY 2016, the OFLC processed 775,979 employer applications for 1,699,957 positions for temporary and permanent labor certifications. This was a nine percent increase in the overall number of processed applications from the previous year. The process of reviewing every case is becoming a tedious task as the number of applicants is increasing every year.

The increasing number of applicants every year calls for a Machine Learning based solution that can help in shortlisting the candidates having higher chances of VISA approval. OFLC has hired the firm EasyVisa for data-driven solutions. You as a data  scientist at EasyVisa have to analyze the data provided and, with the help of a classification model:

* Facilitate the process of visa approvals.
* Recommend a suitable profile for the applicants for whom the visa should be certified or denied based on the drivers that significantly influence the case status.

### Data Description

The data contains the different attributes of employee and the employer. The detailed data dictionary is given below.

* case_id: ID of each visa application
* continent: Information of continent the employee
* education_of_employee: Information of education of the employee
* has_job_experience: Does the employee has any job experience? Y= Yes; N = No
* requires_job_training: Does the employee require any job training? Y = Yes; N = No
* no_of_employees: Number of employees in the employer's company
* yr_of_estab: Year in which the employer's company was established
* region_of_employment: Information of foreign worker's intended region of employment in the US.
* prevailing_wage:  Average wage paid to similarly employed workers in a specific occupation in the area of intended employment. The purpose of the prevailing wage is to ensure that the foreign worker is not underpaid compared to other workers offering the same or similar service in the same area of employment.
* unit_of_wage: Unit of prevailing wage. Values include Hourly, Weekly, Monthly, and Yearly.
* full_time_position: Is the position of work full-time? Y = Full Time Position; N = Part Time Position
* case_status:  Flag indicating if the Visa was certified or denied

## Installing and Importing the necessary libraries
"""

# Installing the libraries with the specified version.
!pip install numpy==1.25.2 pandas==1.5.3 scikit-learn==1.5.2 matplotlib==3.7.1 seaborn==0.13.1 xgboost==2.0.3 -q --user

"""**Note**: *After running the above cell, kindly restart the notebook kernel and run all cells sequentially from the below.*"""

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Library to split data
from sklearn.model_selection import train_test_split

# To oversample and undersample data
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score


# libaries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 100)


# Libraries different ensemble classifiers
from sklearn.ensemble import (
    BaggingClassifier,
    RandomForestClassifier,
    AdaBoostClassifier,
    GradientBoostingClassifier
)

from xgboost import XGBClassifier
from sklearn.tree import DecisionTreeClassifier

# Libraries to get different metric scores
from sklearn import metrics
from sklearn.metrics import (
    confusion_matrix,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
)

# To tune different models
from sklearn.model_selection import RandomizedSearchCV

# To ignore unnecessary warnings
import warnings
warnings.filterwarnings("ignore")

"""## Import Dataset"""

# Uncomment and execute the code snippets below if the data is stored in Google Drive
from google.colab import drive
drive.mount('/content/drive')

visa = pd.read_csv('/content/drive/MyDrive/UT AI ML PG program/Module 3_Advanced Machine Learning/Project/EasyVisa.csv')

#copying data to another variable to avoid making changes to the original data
data = visa.copy()

"""## Overview of the Dataset

#### View the first and last 5 rows of the dataset
"""

#view the first 5 rows of the dataset
data.head()

#view the last 5 rows of the dataset
data.tail()

"""#### Understand the shape of the dataset"""

#check the shape of the dataset
data.shape

"""The dataset has 25480 rows and 12 columns.

#### Check the data types of the columns for the dataset
"""

data.info()

"""The dataset has 9 categorical data types and 3 numerical data types.

## Exploratory Data Analysis (EDA)

#### Let's check the statistical summary of the data
"""

#check the statistical summary of the dataset
data.describe()

"""#### Fixing the negative values in number of employees columns"""

#check the negative values in the employee column
data.loc[data['no_of_employees']<0].shape

"""There are 33 entries in the employees column with negative values."""

#taking the absolute values for number of employees
data['no_of_employees'] = abs(data['no_of_employees'])

"""#### Let's check the count of each unique category in each of the categorical variables"""

#making a list of all categorical variables
cat_cols = list(data.select_dtypes(include='object').columns)

#printing number of count of each unique value in each column
for i in cat_cols:
  print(data[i].value_counts())
  print("-"*50)

#checking the number of unique values
data['case_id'].nunique()

"""There are 25480 unique rows."""

#dropping case_id column from the data
data.drop('case_id', axis=1, inplace=True)

"""### Univariate Analysis"""

def histogram_boxplot(data, feature, figsize=(15, 10), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (15,10))
    kde: whether to show the density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a triangle will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram

# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot

"""#### Observations on education of employee"""

labeled_barplot(data, 'education_of_employee', perc=True)

"""The most commom education level is Bachelor's (40.2%), then Masters (37.8%), High School (13.4%), and Doctorate (8.6%).

#### Observations on region of employment
"""

labeled_barplot(data, 'region_of_employment', perc=True)

"""The Northeast region has the highest number of employments (28.2%), followed by South (27.5%), West (25.8%), Midwest (16.9%), and Island (1.5%).

#### Observations on job experience
"""

labeled_barplot(data, 'has_job_experience', perc=True)

"""More employees have job experience (58.1%) than not (41.9%).

#### Observations on case status
"""

labeled_barplot(data, 'case_status', perc=True)

"""The majority of cases were Certified (66.8%), while a significant number were Denied (33.2%).

### Bivariate Analysis

#### Correlation Check
"""

cols_list = data.select_dtypes(include=np.number).columns.tolist()

plt.figure(figsize=(10, 5))
sns.heatmap(
    data[cols_list].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral"
)
plt.show()

"""It appears that no_of_employees, yr_of_estab, and prevailing_wage have very low correlation with each other.

**Creating functions that will help us with further analysis.**
"""

### function to plot distributions wrt target


def distribution_plot_wrt_target(data, predictor, target):

    fig, axs = plt.subplots(2, 2, figsize=(12, 10))

    target_uniq = data[target].unique()

    axs[0, 0].set_title("Distribution of target for target=" + str(target_uniq[0]))
    sns.histplot(
        data=data[data[target] == target_uniq[0]],
        x=predictor,
        kde=True,
        ax=axs[0, 0],
        color="teal",
        stat="density",
    )

    axs[0, 1].set_title("Distribution of target for target=" + str(target_uniq[1]))
    sns.histplot(
        data=data[data[target] == target_uniq[1]],
        x=predictor,
        kde=True,
        ax=axs[0, 1],
        color="orange",
        stat="density",
    )

    axs[1, 0].set_title("Boxplot w.r.t target")
    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette="gist_rainbow")

    axs[1, 1].set_title("Boxplot (without outliers) w.r.t target")
    sns.boxplot(
        data=data,
        x=target,
        y=predictor,
        ax=axs[1, 1],
        showfliers=False,
        palette="gist_rainbow",
    )

    plt.tight_layout()
    plt.show()

def stacked_barplot(data, predictor, target):
    """
    Print the category counts and plot a stacked bar chart

    data: dataframe
    predictor: independent variable
    target: target variable
    """
    count = data[predictor].nunique()
    sorter = data[target].value_counts().index[-1]
    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(
        by=sorter, ascending=False
    )
    print(tab1)
    print("-" * 120)
    tab = pd.crosstab(data[predictor], data[target], normalize="index").sort_values(
        by=sorter, ascending=False
    )
    tab.plot(kind="bar", stacked=True, figsize=(count + 5, 5))
    plt.legend(
        loc="lower left", frameon=False,
    )
    plt.legend(loc="upper left", bbox_to_anchor=(1, 1))
    plt.show()

"""#### Does higher education increase the chances of visa certification for well-paid jobs abroad?"""

stacked_barplot(data, 'education_of_employee', 'case_status')

"""The stacked barplot showed that employees with Master's and Doctorate degrees have a higher proportion of certified visas compared to those with Bachelor's or High School education.

#### How does visa status vary across different continents?
"""

stacked_barplot(data, 'continent', 'case_status')

"""The stacked barplot indicated that visa certification rates vary across continents. Europe and South America seem to have a higher proportion of denied visas compared to other continents.

#### Does having prior work experience influence the chances of visa certification for career opportunities abroad?
"""

stacked_barplot(data, 'has_job_experience', 'case_status')

"""The stacked barplot demonstrated that having job experience increases the chances of visa certification.

#### Is the prevailing wage consistent across all regions of the US?
"""

plt.figure(figsize=(10, 5))
sns.boxplot(data=data, x='region_of_employment', y='prevailing_wage', palette='gist_rainbow')
plt.show()

"""The boxplot showed that the prevailing wage varies across different regions of the US, with some regions having a wider distribution of wages than others.

#### Does visa status vary with changes in the prevailing wage set to protect both local talent and foreign workers?
"""

distribution_plot_wrt_target(data, 'prevailing_wage', 'case_status')

"""Yes, the visa status does vary with changes in the prevailing wage. As seen in the distribution plots, the distribution of the prevailing wage is different for certified cases compared to denied cases. This suggests that the prevailing wage is indeed an important factor influencing the case status.

#### Does the unit of prevailing wage (Hourly, Weekly, etc.) have any impact on the likelihood of visa application certification?
"""

stacked_barplot(data, 'unit_of_wage', 'case_status')

"""Yes, the unit of prevailing wage does have an impact on the likelihood of visa application certification. The stacked barplot showed that the proportion of certified visas varies depending on the unit of wage (Hourly, Weekly, Monthly, or Yearly). 'Year' had the highest proportion of certified visas compared to other units.

## Data Pre-processing

### Outlier Check
"""

#outlier detection using boxplot
numeric_cols = data.select_dtypes(include=np.number).columns.tolist()
plt.figure(figsize=(20, 15))
for i, variable in enumerate(numeric_cols):
    plt.subplot(4, 4, i + 1)
    plt.boxplot(data[variable])
    plt.tight_layout()
    plt.title(variable)
plt.show()

"""Based on the boxplots generated during the outlier check:

- no_of_employees: This boxplot shows a significant number of outliers on the higher end. The box itself is quite narrow, indicating that the majority of companies have a relatively small number of employees, while a few companies have a very large number of employees, which are represented as points far above the upper whisker.
- yr_of_estab: This boxplot appears to have some outliers on the lower end, indicating a few companies were established much earlier than the majority. The box and whiskers cover a reasonable range of establishment years.
- prevailing_wage: This boxplot also shows a considerable number of outliers on the higher end. Similar to no_of_employees, this suggests that while most prevailing wages fall within a certain range, there are some instances of significantly higher wages, shown as points above the upper whisker.

### Data Preparation for modeling
"""

data['case_status'] = data['case_status'].apply(lambda x: 1 if x == 'Certified' else 0)
X = data.drop('case_status', axis=1) #drop the case status from the data
y = data['case_status']

X = pd.get_dummies(X, drop_first=True) #one hot encoding for categorical variables

#split the dataset into train and valid with a ratio of 7:3
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

#split the dataset into valid and test with a ratio of 9:1
X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size=0.25, random_state=42, stratify=y_valid)

print("Shape of Training set : ", X_train.shape) # Print the shape (number of rows and columns) of the training feature set
print("Shape of Validation set : ", X_valid.shape) # Print the shape of the validation feature set
print("Shape of test set : ", X_test.shape) # Print the shape of the test feature set
print("Percentage of classes in training set:") # Print a header for the class distribution in the training set
print(y_train.value_counts(normalize=True)) # Print the percentage of each class in the training target variable
print("Percentage of classes in validation set:") # Print a header for the class distribution in the validation set
print(y_valid.value_counts(normalize=True)) # Print the percentage of each class in the validation target variable
print("Percentage of classes in test set:") # Print a header for the class distribution in the test set
print(y_test.value_counts(normalize=True)) # Print the percentage of each class in the test target variable

"""## Model Building

### Model Evaluation Criterion

- Choose the primary metric to evaluate the model on
- Elaborate on the rationale behind choosing the metric

First, let's create functions to calculate different metrics and confusion matrix so that we don't have to use the same code repeatedly for each model.
* The `model_performance_classification_sklearn` function will be used to check the model performance of models.
* The `confusion_matrix_sklearn` function will be used to plot the confusion matrix.
"""

# defining a function to compute different metrics to check performance of a classification model built using sklearn


def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {"Accuracy": acc, "Recall": recall, "Precision": precision, "F1": f1,},
        index=[0],
    )

    return df_perf

# defining a function to plot the confusion matrix with percentages
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors) # predict the target variable using the model and predictors
    cm = confusion_matrix(target, y_pred) # calculate the confusion matrix
    labels = np.asarray(
        [
            # format each element of the confusion matrix to display the count and percentage
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2) # reshape the labels to a 2x2 array

    plt.figure(figsize=(6, 4)) # create a figure with a specified size
    sns.heatmap(cm, annot=labels, fmt="") # plot the confusion matrix as a heatmap with annotations
    plt.ylabel("True label") # set the label for the y-axis
    plt.xlabel("Predicted label") # set the label for the x-axis

"""#### Defining scorer to be used for cross-validation and hyperparameter tuning"""

#define the metric
scorer = metrics.make_scorer(metrics.f1_score)

"""**We are now done with pre-processing and evaluation criterion, so let's start building the model.**

### Model building with Original data
"""

models = []  # Empty list to store all the models

# Appending models into the list
models.append(("Bagging", BaggingClassifier(random_state=1)))
models.append(("Random forest", RandomForestClassifier(random_state=1)))
models.append(("GBM", GradientBoostingClassifier(random_state=1)))
models.append(("Adaboost", AdaBoostClassifier(random_state=1)))
models.append(("Xgboost", XGBClassifier(random_state=1, eval_metric="logloss")))
models.append(("dtree", DecisionTreeClassifier(random_state=1)))

results1 = []  # Empty list to store all model's CV scores
names = []  # Empty list to store name of the models


# loop through all models to get the mean cross validated score
print("\n" "Cross-Validation performance on training dataset:" "\n")

for name, model in models:
    kfold = StratifiedKFold(
        n_splits=5, shuffle=True, random_state=1
    )  # set the number of splits.
    cv_result = cross_val_score(
        estimator=model, X=X_train, y=y_train, scoring = scorer,cv=kfold
    )
    results1.append(cv_result)
    names.append(name)
    print("{}: {}".format(name, cv_result.mean()))

print("\n" "Validation Performance:" "\n")

for name, model in models:
    model.fit(X_train,y_train) ## fit the model on original training data

    scores = f1_score(y_valid, model.predict(X_valid))

    print("{}: {}".format(name, scores))

# plotting boxplots for CV scores of all models defined above
fig = plt.figure(figsize=(10, 7))
fig.suptitle("Cross-validation scores")
ax = fig.add_subplot(111)
plt.boxplot(results1)
ax.set_xticklabels(names)
plt.show()

"""- GBM (Gradient Boosting Machine) and Adaboost appear to have the highest median F1-scores among all the models, suggesting they generally perform better in terms of the F1 metric.
- Random Forest and Xgboost also show good median F1-scores, slightly lower than GBM and Adaboost, but still competitive.
- Bagging has a lower median F1-score compared to the boosting models and Random Forest.
- Decision Tree has the lowest median F1-score and the widest spread of scores, indicating it's the least stable and generally performs the worst among the models.
- The boosting models (GBM, Adaboost, Xgboost) seem to have a tighter distribution of scores (smaller boxes and whiskers) compared to Bagging and Decision Tree, suggesting more consistent performance across different cross-validation folds.

Based on these observations, GBM and Adaboost seem to be the most promising models based on their cross-validation performance on the original data. However, it's important to also consider their performance on the validation and test sets, as well as the results from the models trained on oversampled and undersampled data, to make a final decision.

### Model Building with Oversampled data
"""

print("Before OverSampling, counts of label '1': {}".format(sum(y_train == 1)))
print("Before OverSampling, counts of label '0': {} \n".format(sum(y_train == 0)))

# Synthetic Minority Over Sampling Technique
sm = SMOTE(sampling_strategy=1, k_neighbors=5, random_state=1) ## set the k-nearest neighbors
X_train_over, y_train_over = sm.fit_resample(X_train, y_train)


print("After OverSampling, counts of label '1': {}".format(sum(y_train_over == 1)))
print("After OverSampling, counts of label '0': {} \n".format(sum(y_train_over == 0)))


print("After OverSampling, the shape of train_X: {}".format(X_train_over.shape))
print("After OverSampling, the shape of train_y: {} \n".format(y_train_over.shape))

models = []  # Empty list to store all the models

# Appending models into the list
models.append(("Bagging", BaggingClassifier(random_state=1)))
models.append(("Random forest", RandomForestClassifier(random_state=1)))
models.append(("GBM", GradientBoostingClassifier(random_state=1)))
models.append(("Adaboost", AdaBoostClassifier(random_state=1)))
models.append(("Xgboost", XGBClassifier(random_state=1, eval_metric="logloss")))
models.append(("dtree", DecisionTreeClassifier(random_state=1)))

results1 = []  # Empty list to store all model's CV scores
names = []  # Empty list to store name of the models


# loop through all models to get the mean cross validated score
print("\n" "Cross-Validation performance on training dataset:" "\n")

for name, model in models:
    kfold = StratifiedKFold(
        n_splits=5, shuffle=True, random_state=1
    )  # set the number of splits
    cv_result = cross_val_score(
        estimator=model, X=X_train_over, y=y_train_over,scoring = scorer, cv=kfold
    )
    results1.append(cv_result)
    names.append(name)
    print("{}: {}".format(name, cv_result.mean()))

print("\n" "Validation Performance:" "\n")

for name, model in models:
    model.fit(X_train_over,y_train_over) ## fit the model on oversampled training data

    scores = f1_score(y_valid, model.predict(X_valid))

    print("{}: {}".format(name, scores))

# plotting boxplots for CV scores of all models defined above
fig = plt.figure(figsize=(10, 7))
fig.suptitle("Cross-validation scores")
ax = fig.add_subplot(111)
plt.boxplot(results1)
ax.set_xticklabels(names)
plt.show()

"""GBM and Adaboost seem to be the most promising models based on their cross-validation performance on the original data.

### Model Building with Undersampled data
"""

rus = RandomUnderSampler(random_state=1, sampling_strategy=1)
X_train_un, y_train_un = rus.fit_resample(X_train, y_train)


print("Before UnderSampling, counts of label '1': {}".format(sum(y_train == 1)))
print("Before UnderSampling, counts of label '0': {} \n".format(sum(y_train == 0)))


print("After UnderSampling, counts of label '1': {}".format(sum(y_train_un == 1)))
print("After UnderSampling, counts of label '0': {} \n".format(sum(y_train_un == 0)))


print("After UnderSampling, the shape of train_X: {}".format(X_train_un.shape))
print("After UnderSampling, the shape of train_y: {} \n".format(y_train_un.shape))

models = []  # Empty list to store all the models

# Appending models into the list
models.append(("Bagging", BaggingClassifier(random_state=1)))
models.append(("Random forest", RandomForestClassifier(random_state=1)))
models.append(("GBM", GradientBoostingClassifier(random_state=1)))
models.append(("Adaboost", AdaBoostClassifier(random_state=1)))
models.append(("Xgboost", XGBClassifier(random_state=1, eval_metric="logloss")))
models.append(("dtree", DecisionTreeClassifier(random_state=1)))

results1 = []  # Empty list to store all model's CV scores
names = []  # Empty list to store name of the models


# loop through all models to get the mean cross validated score
print("\n" "Cross-Validation performance on training dataset:" "\n")

for name, model in models:
    kfold = StratifiedKFold(
        n_splits=5, shuffle=True, random_state=1
    )  ## set the number of splits
    cv_result = cross_val_score(
        estimator=model, X=X_train_un, y=y_train_un,scoring = scorer, cv=kfold,n_jobs =-1
    )
    results1.append(cv_result)
    names.append(name)
    print("{}: {}".format(name, cv_result.mean()))

print("\n" "Validation Performance:" "\n")

for name, model in models:
    model.fit(X_train_un,y_train_un) ## fit the model on undersampled training data

    scores = f1_score(y_valid, model.predict(X_valid))

    print("{}: {}".format(name, scores))

# plotting boxplots for CV scores of all models defined above
fig = plt.figure(figsize=(10, 7))
fig.suptitle("Cross-validation scores")
ax = fig.add_subplot(111)
plt.boxplot(results1)
ax.set_xticklabels(names)
plt.show()

"""GBM and Adaboost seem to be the most promising models based on their cross-validation performance

## Hyperparameter Tuning

**Best practices for hyperparameter tuning in AdaBoost:**

`n_estimators`:

- Start with a specific number (50 is used in general) and increase in steps: 50, 75, 85, 100

- Use fewer estimators (e.g., 50 to 100) if using complex base learners (like deeper decision trees)

- Use more estimators (e.g., 100 to 150) when learning rate is low (e.g., 0.1 or lower)

- Avoid very high values unless performance keeps improving on validation

`learning_rate`:

- Common values to try: 1.0, 0.5, 0.1, 0.01

- Use 1.0 for faster training, suitable for fewer estimators

- Use 0.1 or 0.01 when using more estimators to improve generalization

- Avoid very small values (< 0.01) unless you plan to use many estimators (e.g., >500) and have sufficient data


---
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # defining model
# model = AdaBoostClassifier(random_state=1)
# 
# # Parameter grid to pass in RandomSearchCV
# param_grid = {
#     "n_estimators": [50, 75, 85, 100], ## set the number of estimators
#     "learning_rate": [1.0, 0.5, 0.1, 0.01], ## set the learning rate.
#     "estimator": [DecisionTreeClassifier(max_depth=1, random_state=1), DecisionTreeClassifier(max_depth=2, random_state=1), DecisionTreeClassifier(max_depth=3, random_state=1),
#     ]
# }
# 
# #Calling RandomizedSearchCV
# randomized_cv = RandomizedSearchCV(
#     estimator=model,
#     param_distributions=param_grid,
#     n_iter=50,
#     n_jobs=-2,
#     scoring=scorer,
#     cv=5, ## set the cv parameter
#     random_state=1
# )
# 
# #Fitting parameters in RandomizedSearchCV
# randomized_cv.fit(X_train_over,y_train_over) ## fit the model on oversampled data

print("Best parameters are {} with CV score={}:" .format(randomized_cv.best_params_,randomized_cv.best_score_))

tuned_ada = randomized_cv.best_estimator_
tuned_ada

tuned_ada.fit(X_train_over, y_train_over)

tuned_ada_train_perf = model_performance_classification_sklearn(tuned_ada, X_train_over, y_train_over)
tuned_ada_train_perf

## check the model performance for validation data.
tuned_ada_val_perf = model_performance_classification_sklearn(tuned_ada,X_valid,y_valid)
tuned_ada_val_perf

"""### Tuning Random forest using Undersampled data

**Best practices for hyperparameter tuning in Random Forest:**


`n_estimators`:

* Start with a specific number (50 is used in general) and increase in steps: 50, 75, 100, 125
* Higher values generally improve performance but increase training time
* Use 100-150 for large datasets or when variance is high


`min_samples_leaf`:

* Try values like: 1, 2, 4, 5, 10
* Higher values reduce model complexity and help prevent overfitting
* Use 1–2 for low-bias models, higher (like 5 or 10) for more regularized models
* Works well in noisy datasets to smooth predictions


`max_features`:

* Try values: `"sqrt"` (default for classification), `"log2"`, `None`, or float values (e.g., `0.3`, `0.5`)
* `"sqrt"` balances between diversity and performance for classification tasks
* Lower values (e.g., `0.3`) increase tree diversity, reducing overfitting
* Higher values (closer to `1.0`) may capture more interactions but risk overfitting


`max_samples` (for bootstrap sampling):

* Try float values between `0.5` to `1.0` or fixed integers
* Use `0.6–0.9` to introduce randomness and reduce overfitting
* Smaller values increase diversity between trees, improving generalization

---
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # defining model
# model = RandomForestClassifier(random_state=1)
# 
# # Parameter grid to pass in RandomSearchCV
# param_grid = {
#     "n_estimators": [50, 75, 100, 125], ## set the number of estimators.
#     "min_samples_leaf": [1, 2, 4, 5, 10], ## set the minimum number of samples in the leaf node.
#     "max_features": ["sqrt", "log2", None], ## set the maximum number of features.
#     "max_samples": [0.6, 0.7, 0.8, 0.9, 1.0], ## set the maximum number of samples.
# }
# 
# 
# #Calling RandomizedSearchCV
# randomized_cv = RandomizedSearchCV(
#     estimator=model,
#     param_distributions=param_grid,
#     n_iter=50,
#     n_jobs=-2,
#     scoring=scorer,
#     cv=5, ## set the cv parameter
#     random_state=1
# )
# 
# #Fitting parameters in RandomizedSearchCV
# randomized_cv.fit(X_train_over,y_train_over) ## fit the model on oversampled data

print("Best parameters are {} with CV score={}:" .format(randomized_cv.best_params_,randomized_cv.best_score_))

tuned_rf = randomized_cv.best_estimator_
tuned_rf

tuned_rf.fit(X_train_over, y_train_over)

tuned_rf_train_perf = model_performance_classification_sklearn(
    tuned_rf, X_train_un, y_train_un
)
tuned_rf_train_perf

## print the model performance on the validation data.
tuned_rf_val_perf = model_performance_classification_sklearn(tuned_rf,X_valid,y_valid)
tuned_rf_val_perf

"""### Tuning with Gradient boosting with Oversampled data

**Best practices for hyperparameter tuning in Gradient Boosting:**

`n_estimators`:

* Start with 100 (default) and increase: 100, 200, 300, 500
* Typically, higher values lead to better performance, but they also increase training time
* Use 200–500 for larger datasets or complex problems
* Monitor validation performance to avoid overfitting, as too many estimators can degrade generalization


`learning_rate`:

* Common values to try: 0.1, 0.05, 0.01, 0.005
* Use lower values (e.g., 0.01 or 0.005) if you are using many estimators (e.g., > 200)
* Higher learning rates (e.g., 0.1) can be used with fewer estimators for faster convergence
* Always balance the learning rate with `n_estimators` to prevent overfitting or underfitting


`subsample`:

* Common values: 0.7, 0.8, 0.9, 1.0
* Use a value between `0.7` and `0.9` for improved generalization by introducing randomness
* `1.0` uses the full dataset for each boosting round, potentially leading to overfitting
* Reducing `subsample` can help reduce overfitting, especially in smaller datasets


`max_features`:

* Common values: `"sqrt"`, `"log2"`, or float (e.g., `0.3`, `0.5`)
* `"sqrt"` (default) works well for classification tasks
* Lower values (e.g., `0.3`) help reduce overfitting by limiting the number of features considered at each split

---
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # defining model
# model = GradientBoostingClassifier(random_state=1)
# 
# ## define the hyper parameters.
# param_grid={
#     "n_estimators": [100, 200, 300], ## set the number of estimators.
#     "learning_rate": [0.1, 0.05, 0.01], ## set the learning rate.
#     "subsample":[0.7, 0.8, 0.9], ## set the value for subsample.
#     "max_features":["sqrt", "log2", None] ## set the value for max_features.
# }
# 
# #Calling RandomizedSearchCV
# randomized_cv = RandomizedSearchCV(
#     estimator=model,
#     param_distributions=param_grid,
#     n_iter=50,
#     n_jobs=-2,
#     scoring=scorer,
#     cv=5, ## set the cv parameter
#     random_state=1
# )
# 
# #Fitting parameters in RandomizedSearchCV
# randomized_cv.fit(X_train_over, y_train_over)

print("Best parameters are {} with CV score={}:" .format(randomized_cv.best_params_,randomized_cv.best_score_))

tuned_gbm = randomized_cv.best_estimator_
tuned_gbm

tuned_gbm_train_perf = model_performance_classification_sklearn(
    tuned_gbm, X_train_over, y_train_over
)
tuned_gbm_train_perf

## Complete the code to print the model performance on the validation data.
tuned_gbm_val_perf = model_performance_classification_sklearn(tuned_gbm,X_valid,y_valid)
tuned_gbm_val_perf

"""### Tuning XGBoost using Oversampled data

**Best practices for hyperparameter tuning in XGBoost:**

`n_estimators`:

* Start with 50 and increase in steps: 50,75,100,125.
* Use more estimators (e.g., 150-250) when using lower learning rates
* Monitor validation performance
* High values improve learning but increase training time

`subsample`:

* Common values: 0.5, 0.7, 0.8, 1.0
* Use `0.7–0.9` to introduce randomness and reduce overfitting
* `1.0` uses the full dataset in each boosting round; may overfit on small datasets
* Values < 0.5 are rarely useful unless dataset is very large

`gamma`:

* Try values: 0 (default), 1, 3, 5, 8
* Controls minimum loss reduction needed for a split
* Higher values make the algorithm more conservative (i.e., fewer splits)
* Use values > 0 to regularize and reduce overfitting, especially on noisy data


`colsample_bytree`:

* Try values: 0.3, 0.5, 0.7, 1.0
* Fraction of features sampled per tree
* Lower values (e.g., 0.3 or 0.5) increase randomness and improve generalization
* Use `1.0` when you want all features considered for every tree


`colsample_bylevel`:

* Try values: 0.3, 0.5, 0.7, 1.0
* Fraction of features sampled at each tree level (i.e., per split depth)
* Lower values help in regularization and reducing overfitting
* Often used in combination with `colsample_bytree` for fine control over feature sampling

---
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # defining model
# model = XGBClassifier(random_state=1,eval_metric='logloss')
# 
# ## define the hyperparameters
# param_grid={
#     'n_estimators':[50, 75, 100, 125], ## set the number of estimators.
#     'subsample':[0.5, 0.7, 0.8, 1.0], ## set the subsample.
#     'gamma':[0, 1, 3, 5], ## set the gamma.
#     'colsample_bytree':[0.3, 0.5, 0.7, 1.0], ## set the value for colsample_bytree.
#     'colsample_bylevel':[0.3, 0.5, 0.7, 1.0], ## set the value for colsample_bylevel.
# }
# 
# #Calling RandomizedSearchCV
# randomized_cv = RandomizedSearchCV(
#     estimator=model,
#     param_distributions=param_grid,
#     n_iter=50,
#     n_jobs=-2,
#     scoring=scorer,
#     cv=5, ## set the cv parameter
#     random_state=1
# )
# 
# #Fitting parameters in RandomizedSearchCV
# randomized_cv.fit(X_train_over,y_train_over)## fit the model on oversampled data

print("Best parameters are {} with CV score={}:" .format(randomized_cv.best_params_,randomized_cv.best_score_))

tuned_xgb = randomized_cv.best_estimator_
tuned_xgb

tuned_xgb_train_perf = model_performance_classification_sklearn(
    tuned_xgb, X_train_over, y_train_over
)
tuned_xgb_train_perf

## print the model performance on the validation data.
tuned_xgb_val_perf = model_performance_classification_sklearn(tuned_xgb,X_valid,y_valid)
tuned_xgb_val_perf

"""## Model Performance Summary and Final Model Selection"""

# training performance comparison

models_train_comp_df = pd.concat(
    [
        tuned_gbm_train_perf.T,
        tuned_xgb_train_perf.T,
        tuned_ada_train_perf.T,
        tuned_rf_train_perf.T,
    ],
    axis=1,
)
models_train_comp_df.columns = [
    "Gradient Boosting tuned with oversampled data",
    "XGBoost tuned with oversampled data",
    "AdaBoost tuned with oversampled data",
    "Random forest tuned with undersampled data",
]
print("Training performance comparison:")
models_train_comp_df

# validation performance comparison

models_val_comp_df = pd.concat(
    [
        tuned_gbm_val_perf.T,
        tuned_xgb_val_perf.T,
        tuned_ada_val_perf.T,
        tuned_rf_val_perf.T,
    ],
    axis=1,
)
models_val_comp_df.columns = [
    "Gradient Boosting tuned with oversampled data",
    "XGBoost tuned with oversampled data",
    "AdaBoost tuned with oversampled data",
    "Random forest tuned with undersampled data",
]
print("Validation performance comparison:")
models_val_comp_df

"""XGBoost achieved the highest cross-validation F1-score during tuning on the oversampled data, and also performed slightly better than the other models on the validation set in terms of F1-score."""

# selecting the final model based on the F1-score on the validation set

final_model = tuned_xgb

test = model_performance_classification_sklearn(final_model, X_test, y_test)
test

feature_names = X_train.columns
importances = final_model.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12, 12))
plt.title("Feature Importances")
plt.barh(range(len(indices)), importances[indices], color="violet", align="center")
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel("Relative Importance")
plt.show()

"""This plot shows the relative importance of each feature in your final XGBoost model. The longer the bar, the more important the feature is in predicting the visa case status.

Looking at the plot, we can see that:

- prevailing_wage is by far the most important feature in determining the visa case status. This aligns with our earlier bivariate analysis, where we saw different distributions of prevailing wage for certified and denied cases.
- yr_of_estab and no_of_employees also have some importance, but significantly less than prevailing_wage.
- Categorical features that were one-hot encoded, such as continent, education_of_employee, region_of_employment, unit_of_wage, has_job_experience, and full_time_position, also contribute to the model's predictions, with some categories being more important than others. For example, continent_Asia and education_of_employee_High School appear to have higher importance among the categorical features.

These insights suggest that while prevailing_wage is the dominant factor, other features, both numerical and categorical, play a role in the model's decision-making process. This information can be valuable for understanding the key drivers of visa certification and for making recommendations.

## Actionable Insights and Recommendations

Actionable Insights:

1. Prevailing Wage is the Most Critical Factor: The feature importance plot clearly shows that prevailing_wage is the most significant factor influencing visa certification. This highlights the importance of ensuring that the offered wage meets or exceeds the prevailing wage for the occupation and location.
2. Education and Experience Matter: Employees with higher education levels (Master's and Doctorate) and those with prior job experience have a higher likelihood of visa certification. This suggests that the US prioritizes highly skilled and experienced foreign workers.
3. Geographic Variations Exist: Visa certification rates and prevailing wages vary across different regions of the US. This indicates that the demand for foreign workers and wage standards are not uniform across the country.
4. Unit of Wage Influences Outcomes: The unit in which the prevailing wage is specified (Year, Hour, Week, Month) impacts the certification likelihood, with 'Year' having the highest proportion of certified cases. This might be due to how annual salaries are perceived or processed in the certification system.
5. Imbalance in Case Status: The dataset shows a significant imbalance between certified and denied cases, with a much higher number of certified visas. This is important to consider when building and evaluating models to avoid bias towards the majority class.

Recommendations:

1. Focus on Prevailing Wage Compliance: EasyVisa should emphasize to employers the critical importance of offering competitive wages that meet or exceed the prevailing wage. They could develop tools or guidelines to help employers accurately determine the prevailing wage for their specific job requirements and locations.
2. Target Skilled and Experienced Applicants: EasyVisa could focus its efforts on attracting and assisting applicants with higher education and relevant job experience, as they have a greater chance of certification. Marketing and outreach efforts could be tailored to these demographics.
3. Provide Region-Specific Guidance: Given the variations across US regions, EasyVisa should provide region-specific guidance to both employers and employees regarding prevailing wages, demand for certain occupations, and potential differences in the application process or requirements.
4. Clarify Wage Unit Requirements: OFLC could provide clearer guidelines or preferences regarding the unit of prevailing wage in applications to ensure consistency and potentially streamline the process. EasyVisa should advise employers to use the 'Year' unit where appropriate, given its higher certification rate in the historical data.
5. Utilize the Predictive Model: The developed classification model can be used by EasyVisa to pre-assess the likelihood of visa certification for applicants. This can help in:
- Prioritizing applications: Focusing on applications with a higher predicted chance of approval.
- Identifying potential issues: Flagging applications with a lower predicted chance for further review or to advise employers/employees on strengthening their application (e.g., adjusting the proposed wage).
- Managing expectations: Providing applicants and employers with a realistic assessment of their chances.
6. Continuous Monitoring and Feedback: OFLC and EasyVisa should continuously monitor the performance of the model and the visa certification process. Feedback from denied cases, especially those with high predicted probabilities of certification, can help refine the model and identify potential bottlenecks or inconsistencies in the manual review process.

These insights and recommendations can help EasyVisa and OFLC optimize their processes, improve efficiency, and increase the likelihood of successful visa certifications for qualified foreign workers while ensuring compliance with regulations.

<font size=6 color='blue'>Power Ahead</font>
___
"""